{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from numpy.core.multiarray import ndarray\n",
    "from numpy.linalg import inv\n",
    "from typing import Optional\n",
    "\n",
    "np.random.seed(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression (analytical solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{y}$ - vector with target variable values  \n",
    "\n",
    "$\\mathbf{w}$ - vector with linear regression weights\n",
    "\n",
    "$\\mathbf{X}$ - train matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\mathbf{e} $ \n",
    "\n",
    "$ E = \\sum_{i=1}^{N} ( \\mathbf{x}_i^{T} \\mathbf{w} - y_i ) ^2 = (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) ^2 = (\\mathbf{X} \\mathbf{w} - \\mathbf{y} ) ^T (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) = (\\mathbf{w}^T\\mathbf{X}^T - \\mathbf{y}^T)(\\mathbf{X} \\mathbf{w} - \\mathbf{y}) = \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\mathbf{w} + \\mathbf{y}^T\\mathbf{y} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial E}{\\partial w} = \\mathbf{w}^T(\\mathbf{X}^T\\mathbf{X} + \\mathbf{X}^T\\mathbf{X}) - (\\mathbf{X}^T\\mathbf{y})^T - \\mathbf{y}^T\\mathbf{X} + 0 = 2\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equate the derivative to zero and solve the resulting equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ 2\\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} = 0 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X} = \\mathbf{y}^T\\mathbf{X} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{X}^T\\mathbf{X}\\mathbf{w} = \\mathbf{X}^T\\mathbf{y} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionAnalytical:\n",
    "    def __init__(self) -> None:\n",
    "        self.weights = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X_train: ndarray, y: ndarray) -> None:\n",
    "        X = np.hstack((X_train, np.ones((len(X_train), 1))))\n",
    "        coefs = inv(X.T @ X) @ X.T @ y\n",
    "        self.weights = coefs[:-1]\n",
    "        self.intercept = coefs[-1]\n",
    "\n",
    "    def predict(self, X_test: ndarray) -> ndarray:\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"This linear regression instance is not fitted yet.\")\n",
    "        y_pred = X_test @ self.weights + self.intercept\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression (stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y $ - predicted target variable\n",
    "\n",
    "$ t $ - target variable\n",
    "\n",
    "$ \\mu $ - learning rate\n",
    "\n",
    "$ \\mathbf{w} $ - vector with linear regression weights\n",
    "\n",
    "$ N $ - batch size\n",
    "\n",
    "$  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf{w}(i+1) = \\mathbf{w}(i) - \\Delta \\mathbf{w}(i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Delta \\mathbf{w} = \\mu \\frac{\\partial \\xi}{\\partial \\mathbf{w}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial \\xi_i}{\\partial \\mathbf{w}} = \\frac{\\partial \\xi_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\mathbf{w}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial \\xi_i}{\\partial y_i} = \\frac{\\partial (t_i - y_i)^2}{\\partial y_i} = - 2 (t_i - y_i) = 2 (y_i - t_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial y_i}{\\partial \\mathbf{w}} = \\frac{\\partial (\\mathbf{x}_i \\cdot \\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{x}_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Delta \\mathbf{w} = \\mu \\cdot \\frac{\\partial \\xi_i}{\\partial \\mathbf{w}} = \\mu \\cdot 2 \\mathbf{x}_i (y_i - t_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Delta \\mathbf{w} = 2 * \\mu * \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i (y_i - t_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to use a variable learning rate: first, when the minimum point is still far away, move quickly, and later, after a certain number of iterations, take slower steps.\n",
    "One way to set the step size is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mu_t = \\frac{\\mu}{t} $\n",
    "\n",
    "$ \\mu_t $ - learning rate at epoch t\n",
    "\n",
    "$ \\mu $ - initial learning rate\n",
    "\n",
    "$ t $ - epoch number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be many conditions for stopping the algorithm learning:\n",
    "- reaching the specified number of iterations;\n",
    "- euclidean distance between weights in two adjacent iterations below a certain threshold;\n",
    "- the last n iterations, the loss function has not improved\n",
    "- and etc.\n",
    "In this implementation, I will use the first two criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_distance(vec_1: ndarray, vec_2: ndarray) -> float:\n",
    "    return np.sqrt(np.power(vec_1 - vec_2, 2).sum())\n",
    "\n",
    "\n",
    "def mse(y_true: ndarray, y_pred: ndarray) -> float:\n",
    "    return np.power(y_true - y_pred, 2).mean()\n",
    "\n",
    "\n",
    "class LinearRegressionSGD:\n",
    "    def __init__(self, learning_rate: float) -> None:\n",
    "        self.weights = None\n",
    "        self.intercept = None\n",
    "        self.errors = None\n",
    "        self.lr = learning_rate\n",
    "        self.errors = list()\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ndarray,\n",
    "        y: ndarray,\n",
    "        n_epochs: int,\n",
    "        min_weight_dist: float = 1e-8,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        n_samples = len(y)\n",
    "        if batch_size is None:\n",
    "            batch_size = n_samples\n",
    "\n",
    "        X = np.hstack((X_train, np.ones((len(X_train), 1))))\n",
    "        w = np.random.normal(size=X_train.shape[1] + 1)\n",
    "        w_prev = w + min_weight_dist * 10e5\n",
    "\n",
    "        epoch_num = 0\n",
    "\n",
    "        while euclid_distance(w_prev, w) > min_weight_dist and epoch_num < n_epochs:\n",
    "            epoch_num += 1\n",
    "            cur_lr = self.lr / epoch_num\n",
    "            inds = np.random.choice(np.arange(n_samples), size=min(batch_size, n_samples), replace=False)\n",
    "\n",
    "            y_pred = X[inds] @ w\n",
    "            y_true = y[inds]\n",
    "            N = len(inds)\n",
    "            gradient_step = (X[inds] * (y_pred - y_true).reshape((-1, 1))).sum(axis=0) * cur_lr / N\n",
    "            w_prev = w\n",
    "            w = w - gradient_step\n",
    "            self.errors.append(mse(y, X @ w))\n",
    "\n",
    "        self.weights = w[:-1]\n",
    "        self.intercept = w[-1]\n",
    "\n",
    "    def predict(self, X_test: ndarray) -> ndarray:\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"This linear regression instance is not fitted yet.\")\n",
    "        y_pred = X_test @ self.weights + self.intercept\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
